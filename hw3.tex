\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{fancyhdr}
\usepackage{epsfig}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}


% FILL IN THE SPECIFICS OF EACH HOMEWORK HERE
\newcommand{\course}{CS 364}
\newcommand{\semester}{Spring 2013}
\newcommand{\name}{Artificial Intelligence}
%%%
%%%
%%% PLEASE FILL OUT YOUR NAME AND THE HWK NUMBER
%%%
%%%
\newcommand{\hwk}{Homework \#3 Solutions}
\newcommand{\student}{Oren Shoham, Peter Fogg, Sayer Rippey}

\newtheorem{lemma}{Lemma}
\newtheorem*{lem}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{notation}{Notation}
\newtheorem*{claim}{Claim}
\newtheorem*{fclaim}{False Claim}
\newtheorem{observation}{Observation}
\newtheorem{conjecture}[lemma]{Conjecture}
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem*{rt}{Running Time}




%%% You can ignore the following stuff, it's just for formatting purposes
\textheight=8.6in
\setlength{\textwidth}{6.44in}
\addtolength{\headheight}{\baselineskip} 
% enumerate uses a), b), c), ...
\renewcommand{\labelenumi}{\alph{enumi})}
% Sets the style for fancy pages (namely, all but first page)
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0.0pt}
\renewcommand{\footrulewidth}{0.4pt}
% Changes style of plain pages (namely, the first page)
\fancypagestyle{plain}{
  \fancyhf{}
  \renewcommand\headrulewidth{0pt}
  \renewcommand\footrulewidth{0.4pt}
  \renewcommand{\headrule}{}
}
% Changes the title box on the first page
\renewcommand\maketitle{
  \begin{center}
    \begin{tabular*}{6.44in}{l @{\extracolsep{\fill}}c r}
      \bfseries  &  & \bfseries \course ~\semester \\
      \bfseries&  & \bfseries  \hwk  \\
      \bfseries   &   &  \bfseries \student \\ 
    \end{tabular*}
\end{center} }




%%
%%
%% THE REAL STUFF STARTS HERE
%%
%%
\begin{document}
\maketitle
\thispagestyle{plain}


%%% PLEASE PLACE THE HONOR CODE AND YOUR NAME/SIGNATURE HERE
\noindent Honor Code: 

\subsection*{Part 1}
\begin{enumerate}
\item This is wrong. Before we knew anything, B and C had a collective $\frac{2}{3}$ probability of being executed -- we sum their individual probability. Now that B is definitely not executed, C has that entire $\frac{2}{3}$, while A's probability is unchanged.
\item This is right. Let GSBP denote the event that the guard says B is pardoned, $A$ denote the event that $A$ is executed, and similarly for $B$ and $C$.
  \begin{align*}
    P(A\ |\ \text{GSBP}) &= \frac{P(\text{GSBP}\ |\ A)P(A)}{P(\text{GSBP})} \\
    &= \frac{P(\text{GSBP}\ |\ A)P(A)}{P(\text{GSBP}\ |\ A)P(A) + P(\text{GSBP}\ |\ B)P(B) + P(\text{GSBP}\ |\ C)P(C)} \\
    &= \frac{\frac{1}{2} \cdot \frac{1}{3}}{\frac{1}{2} \cdot \frac{1}{3} + 0 + 1 \cdot \frac{1}{3}} \\
    &= \frac{1}{3}
  \end{align*}
  If A is executed, then B and C have both been pardoned. So the guard will say that B has been pardoned with probability $\frac{1}{2}$. However, if C is executed, then the guard will say B is pardoned with a probability of 1.
%%   \begin{align*}
%%     P(A \text{ exec.}\ |\ B \text{ not exec.}) &= \frac{P(\text{$A$ exec.})P(\text{$B$ not exec. $\ |\ A$ exec.})}{P(\text{$B$ not exec.})} & \text{(1)} \\
%%     &= \frac{\frac{1}{3} \cdot 1}{\frac{2}{3}} & \text{(2)} \\
%%     &= \frac{1}{2}
%%   \end{align*}
%%   \begin{enumerate}[(1)]
%%     \item This is just an application of Bayes' theorem. Now that we know $B$ will live, we can use this to figure out $A$'s probability of being executed.
%%     \item We know that only one person is being executed, so if $A$ is executed then the probability of $B$ living is 1.
%%   \end{enumerate}
%% \item This explanation supposes that the probability of $A$ being executed is independent of the probabilities of the other prisoners being executed. This is wrong. What if we had learned that $B$ \emph{was} going to be executed? Then clearly $A$ would be stupid to think that they have a $\frac{1}{3}$ probability of being executed.
\end{enumerate}
\subsection*{Part 2}
We want to find $P(hwDone|scoutsHonor)$, the probability that a student's homework has been completed given the fact that they said it had (and they forgot it at home). We know that if a student's homework is completed, they will say they forgot it at home with .01 probability, and we guess that if it isn't completed, they'll say they forgot it with .5 probability. Finally, we guess about 90\% of the students did the homework. With Bayes' rule, we have:
 \begin{align*}
P(hwD|sH) &= P(sH|hwD) *P(hwD) / P(sH) \\
&= (.01) * (.90) / (P(hwD)*P(sH|hwD) + P(\overline{hwD})*P(sH| \overline{hwD})) \\
&= (.01) * (.90) / ((.90*.01)+(.10*.50))\\
&= .15\\
\end{align*}
\subsection*{Part 3}
\begin{enumerate}[a.]
\item \begin{align*}
P(A \land B \land C \land D \land E) &= P(A) * P(B) * P(C) * P(D|A,B) * P(E|B,C)\\
&= .2 * .5 * .8 * .1 * .3\\
&= .0024\\
\end{align*}
\item \begin{align*}
P(\lnot A \land \lnot B \land \lnot  C \land \lnot D \land \lnot E) &= P(\overline{A}) * P(\overline{B}) * P(\overline{C}) * P(\overline{D}|\overline{A},\overline{B}) * P(\overline{E}|\overline{B},\overline{C})\\
&= .8 * .5 * .2 * .1 * .8\\
&= .0064\\
\end{align*}
\item \begin{align*}
P(\lnot A \land B \land C \land D \land E) &= P(\overline{A}) * P(B) * P(C) * P(D|\overline{A},B) * P(E|B,C)\\
&= .8 * .5 * .8 * .6 * .3\\
&= .0576\\
\end{align*}
\end{enumerate}
\subsection*{Part 4}
Ha ha!
\subsection*{Part 5}
\begin{itemize}
\item Natural-looking facial animations can be used in a variety of contexts, from computer games to studies of speech perception. Xue \footnote{Jianxia Xue. ``Acoustically-Driven Talking Face Animations Using Dynamic Bayesian Networks.'' (2008).} used Bayesian networks to develop a system for constructing facial animations from audio data. A two-dimensional hidden state space was used to relate the audio and facial data to in order to construct the most likely facial motion for a given sequence of sounds. This allowed the synthesis of facial animations from arbitrary human speech. The resultant animations were fairly accurate. A study was made with sixteen participants, asking them to decide whether a pair of animation and sound matched, or were different. The participants answered correctly around 80\% of the time.
\item In most modern videogames, narratives are told in a predetermined, linear fashion, and the player's actions have very little or no impact on the story being told. In an effort to provide an alternative to this storytelling paradigm, Bangs\o \ \emph{et al.} \footnote{Olav Bangso, Ole G. Jensen, Finn V. Jensen, Peter B. Andersen, and Tomas Kocka, “Non-linear interactive storytelling using object-oriented bayesian networks.”} developed a framework for generating non-linear narratives in videogames that are determined in a meaningful way by player interaction (called NOLIST, for non-linear interactive storytelling). This system uses Bayesian networks to model the probability of potential game narratives based on events that have already occurred or could potentially occur in the game, and uses this information to construct coherent narratives as the game progresses. \\
\\The NOLIST framework treats a narrative as a set of \emph{actions}, which are defined by their content, prerequisites, and effects, where an action's content is a set of other, smaller actions, its prerequisites are events that must occur before the action can be performed, and its effects are the events that occur as a result of the action being performed. \\
\\ The authors use a special type of Bayesian network known as an Object-Oriented Bayesian network to represent the story of a game, where nodes are "classes" that represent actions and their associated fields (for a thorough explanation of Object-Oriented Bayesian networks, see this paper\footnote{Koller, Daphne, and Avi Pfeffer. "Object-oriented Bayesian networks." Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence. Morgan Kaufmann Publishers Inc., 1997.} by D. Koller and A. Pfeffer). If a particular action is performed, then probabilities within the network are updated to reflect its effects and any actions it might be a prerequisite for. Given a network like this, a game engine could in theory choose a fairly coherent, well-structured narrative over the course of a game in response to a player's interactions by choosing actions with high probabilities. As far as we can tell, NOLIST hasn't been implemented in any game engines yet, so there isn't really any evidence one way or the other that it would work as intended. 
\end{itemize}
\subsection*{Part 6}
\begin{itemize}
\item \textbf{Where Should I Get Coffee?} Suppose you're trying to get some coffee in Oberlin. Do you go to Slow Train, or the Local? It depends on how many people are there, and whether or not you can get a table. The problem is how to find out how many people are where. We propose to aggregate data from various social media platforms and mobile apps (Twitter, Foursquare, etc.), and use a Bayesian network to determine which place is less crowded.

We will use geocoding information to filter data from a particular location. Consider a single person. The probability that they are in a particular coffee shop can be determined with evidence such as whether or not they've recently tweeted about coffee, their most recent Foursquare checking, etc. We combine all of this data using a Bayesian network, and, if the person is likely to be at a particular location, we add them to that shop's total. We can account for the fact that not everyone is on Twitter by multiplying the sum by a constant, or even weighting each person by their probability of being there.

This could be easily extended to other businesses -- should we go to the Feve or Black River for dinner?
\item
\end{itemize}
\end{document}
